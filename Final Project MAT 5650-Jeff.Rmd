---
title: "STA 5650 Final Project"
author: "Jeff Robbins"
date: "Due 12/4/22"
output:
    pdf_document:
      df_print: kable
---

```{r global_options, include=FALSE}
# these are some optional settings that will change how some features look
# you do not need to change them.
knitr::opts_chunk$set(out.width = "50%", out.height="50%", fig.align="center", warning=FALSE, message=FALSE)
```


### Loading Relevant Packages

```{r}
# load any relevant packages here, if necessary
library(clustMixType)
library(factoextra)
library(ggplot2)
library(caret)
library(lattice)
library(dplyr)
library(cluster)
```


```{r}
save(Wine_Df, file = "Wine_Df")
load(file = "Wine_Df")

# summary(Wine_Df)

# Scale the data 
Wine_scaled <- scale(Wine_Df)
summary(Wine_scaled)
# plot(Wine_scaled)

```



```{r}


# Calculate distance metrics 
#     Why calculate distance?
Distance_wine <- dist(Wine_scaled)

# Calculate how many clusters to use, using the 'elbow' plot
# wss = within sum squares
fviz_nbclust(Wine_scaled, kmeans, method = 'wss') + 
  labs(subtitle = "Elbow Method")
# Looks like 3 - 6 clusters, likely 3


# Can also see the optimal number of clusters using the silhouette score
fviz_nbclust(Wine_scaled, kmeans, method = "silhouette", k.max = 20)

# dim(Wine_scaled) is 178 x 13

# K means clustering
kmeans_out <- kmeans(Wine_scaled, centers = 3, nstart = 10000)
print(kmeans_out)

kmeans_out$cluster

# Visualize cluster algorithm results
km_cluster <- kmeans_out$cluster

Wine_Clustered <- data.frame(Wine_scaled, km_cluster)

fviz_cluster(list(data = Wine_Clustered, cluster = km_cluster))

hist(Wine_Clustered$km_cluster,Wine_Clustered$Alcohol)

Wine_Clustered$km_cluster <- as.factor(Wine_Clustered$km_cluster)

hist(Wine_Clustered$km_cluster,Alcohol)

Wine_Clustered$km_cluster <- as.numeric(Wine_Clustered$km_cluster)
typeof(Wine_Clustered$km_cluster)

ggplot(Wine_Clustered) +
             geom_boxplot(mapping = aes(x=km_cluster, y=Alcohol))

Wine_Clustered$km_cluster <- as.factor(Wine_Clustered$km_cluster)


# Single scatterplot colored by cluster
ggplot(Wine_Clustered, aes(Alcohol, y = Flavanoids, 
                           color = Wine_Clustered$km_cluster)) + 
  geom_point() +
  guides(color = guide_legend(title = 'Cluster'))

# Gathering column names / variable names
ColNames <- colnames(Wine_Clustered)
ColNames[2]
  
# Plotting all pairs of scatterplots and coloring by cluster, output of 79 plots
for (i in 1:13)(
  for(j in (i+1):13)(
        print(ggplot(Wine_Clustered, aes(Wine_Clustered[,i], y = Wine_Clustered[,j], 
                               color = km_cluster)) + 
            geom_point() +
            guides(color = guide_legend(title = 'Cluster')) +
          labs(x = ColNames[i],
               y = ColNames[j]))
  )
)

Wine_Clustered <- as.data.frame(Wine_Clustered)

# Single boxplot graph by cluster
ggplot(data = Wine_Clustered) +
  geom_boxplot(mapping = aes(x = km_cluster, 
                             y = Alcohol))

# Every possible boxplot by catagory cluster, output 13 plots
for(i in 1:13)(
  print(ggplot(data = Wine_Clustered) +
           geom_boxplot(mapping = aes(x = km_cluster, 
                             y = Wine_Clustered[,i])) +
            labs(x = km_cluster, y = ColNames[i]))
)

```

```{r}

# k-medoids clustering
K_med <- pam(Wine_scaled, 3)
K_med$medoids

fviz_cluster(K_med, data = Wine_scaled, palette = 'jco', 
             ggtheme = theme_minimal(), labelsize = 2)
```




```{r}
# Stat_quest github code example

## In this example, the data is in a matrix called
## data.matrix
## columns are individual samples (i.e. cells)
## rows are measurements taken for all the samples (i.e. genes)
## Just for the sake of the example, here's some made up data...

 
pca <- prcomp(Wine_scaled, scale=FALSE) 
 
## plot pc1 and pc2
plot(pca$x[,1], pca$x[,2])
plot(pca$x[,2], pca$x[,3])
 
## make a scree plot
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
 
barplot(pca.var.per, main="Scree Plot", xlab="Principal Component", 
        ylab="Percent Variation")
 
## now make a fancy looking plot that shows the PCs and the variation:

pca$x <- as.data.frame(pca$x)
 
pca.data <- data.frame(Sample=rownames(pca$x),
  X=pca$x[,1],
  Y=pca$x[,2])
pca.data
 
ggplot(data=pca.data, aes(x=X, y=Y, label=Sample)) +
  geom_text() +
  xlab(paste("PC1 - ", pca.var.per[1], "%", sep="")) +
  ylab(paste("PC2 - ", pca.var.per[2], "%", sep="")) +
  theme_bw() +
  ggtitle("My PCA Graph")
 
## get the name of the top 10 measurements (genes) that contribute
## most to pc1.
loading_scores <- pca$rotation[,1]
gene_scores <- abs(loading_scores) ## get the magnitudes
gene_score_ranked <- sort(gene_scores, decreasing=TRUE)
top_10_genes <- names(gene_score_ranked[1:10])
 
top_10_genes ## show the names of the top 10 genes
 
pca$rotation[top_10_genes,1] ## show the scores (and +/- sign)
 
#######
##
## NOTE: Everything that follow is just bonus stuff.
## It simply demonstrates how to get the same
## results using "svd()" (Singular Value Decomposition) or using "eigen()"
## (Eigen Decomposition).
##
#######
 
############################################
##
## Now let's do the same thing with svd()
##
## svd() returns three things
## v = the "rotation" that prcomp() returns, this is a matrix of eigenvectors
##     in other words, a matrix of loading scores
## u = this is similar to the "x" that prcomp() returns. In other words,
##     sum(the rotation * the original data), but compressed to the unit vector
##     You can spread it out by multiplying by "d"
## d = this is similar to the "sdev" value that prcomp() returns (and thus
##     related to the eigen values), but not
##     scaled by sample size in an unbiased way (ie. 1/(n-1)).
##     For prcomp(), sdev = sqrt(var) = sqrt(ss(fit)/(n-1))
##     For svd(), d = sqrt(ss(fit))
##
############################################
 
svd.stuff <- svd(scale(t(data.matrix), center=TRUE))
 
## calculate the PCs
svd.data <- data.frame(Sample=colnames(data.matrix),
  X=(svd.stuff$u[,1] * svd.stuff$d[1]),
  Y=(svd.stuff$u[,2] * svd.stuff$d[2]))
svd.data
 
## alternatively, we could compute the PCs with the eigen vectors and the
## original data
svd.pcs <- t(t(svd.stuff$v) %*% t(scale(t(data.matrix), center=TRUE)))
svd.pcs[,1:2] ## the first to principal components
 
svd.df <- ncol(data.matrix) - 1
svd.var <- svd.stuff$d^2 / svd.df
svd.var.per <- round(svd.var/sum(svd.var)*100, 1)
 
ggplot(data=svd.data, aes(x=X, y=Y, label=Sample)) +
  geom_text() +
  xlab(paste("PC1 - ", svd.var.per[1], "%", sep="")) +
  ylab(paste("PC2 - ", svd.var.per[2], "%", sep="")) +
  theme_bw() +
  ggtitle("svd(scale(t(data.matrix), center=TRUE)")
 
############################################
##
## Now let's do the same thing with eigen()
##
## eigen() returns two things...
## vectors = eigen vectors (vectors of loading scores)
##           NOTE: pcs = sum(loading scores * values for sample)
## values = eigen values
##
############################################
cov.mat <- cov(scale(t(data.matrix), center=TRUE))
dim(cov.mat)
 
## since the covariance matrix is symmetric, we can tell eigen() to just
## work on the lower triangle with "symmetric=TRUE"
eigen.stuff <- eigen(cov.mat, symmetric=TRUE)
dim(eigen.stuff$vectors)
head(eigen.stuff$vectors[,1:2])
 
eigen.pcs <- t(t(eigen.stuff$vectors) %*% t(scale(t(data.matrix), center=TRUE)))
eigen.pcs[,1:2]
 
eigen.data <- data.frame(Sample=rownames(eigen.pcs),
  X=(-1 * eigen.pcs[,1]), ## eigen() flips the X-axis in this case, so we flip it back
  Y=eigen.pcs[,2]) ## X axis will be PC1, Y axis will be PC2
eigen.data
 
eigen.var.per <- round(eigen.stuff$values/sum(eigen.stuff$values)*100, 1)
 
ggplot(data=eigen.data, aes(x=X, y=Y, label=Sample)) +
  geom_text() +
  xlab(paste("PC1 - ", eigen.var.per[1], "%", sep="")) +
  ylab(paste("PC2 - ", eigen.var.per[2], "%", sep="")) +
  theme_bw() +
  ggtitle("eigen on cov(t(data.matrix))")


```





```{r}

# Agglomeritave Heirarchical clustering

install.packages('cluster')
library(cluster)



# Use the hclust function to get heirarchical clustering
#   dist() is the chosen metric
#   method is used to define a similarity measure foe the heirarchical #     clustering

heir_cluster <- hclust(dist(Wine_scaled), method = "complete")

# Plot results in a dendogram
plot(heir_cluster)


# Now determine where to cut the tree based on how many clusters we want, 3

ClusterCut <- cutree(heir_cluster, 3)


#########################################################
# Use Single as a similarity metric

heir_clust_single <- hclust(dist(Wine_scaled), method = "single")

plot(heir_clust_single)

# Divide into 3 clusters

ClusterCut_single <- cutree(heir_clust_single, 3)
table(ClusterCut_single)

########################################################

# Now, use the average similarity metric

h.clust_avg <- hclust(dist(Wine_scaled), method = "average")

plot(h.clust_avg)

ClusterCut_avg <- cutree(h.clust_avg, 3)

table(ClusterCut_avg)

#######################################################

# Now, use the centroid metric

h.clust_cent <- hclust(dist(Wine_scaled), method = "centroid")

plot(h.clust_cent)

ClusterCut_cent <- cutree(h.clust_cent, 3)

table(ClusterCut_cent)

```


```{r}

# Divisive heirarchical clustering

# Measure dissimilarity by the Diana metric

h.clust_diana <- diana(Wine_scaled, metric = "euclidean")

pltree(h.clust_diana, cex = 0.6, main = "Dendrogram of divisive clustering")

ClusterCut_diana <- cutree(h.clust_diana, 3)

table(ClusterCut_diana)


fviz_cluster(list(data = Wine_scaled, cluster = ClusterCut_diana))

```



```{r}

install.packages("dbscan")
library(dbscan) 


# Calculate and store clusters in the cluster variable, and choose values of epsilon and minpts


Cluster_db <- dbscan(Wine_scaled, eps = seq(0.01,5,0.01), minPts = 5)


fviz_cluster(Cluster_db, Wine_scaled, geom = "point", 
             palette = "set2", ggtheme = theme_minimal())

``` 




```{r}

# PCA 
#   Roughly page 177
Wine_cov <- cov(Wine_scaled)

Wine_eigen <- eigen(Wine_cov)


# Eigen vectors are the principal componeents
print(Wine_eigen$vectors)
print(Wine_eigen$values)

plot(Wine_eigen$valueS, type = 'o')

```

