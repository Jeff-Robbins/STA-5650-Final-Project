---
title: "STA5650FinalProject_MA"
author: "Monica Amezquita"
date: "2022-12-06"
output: pdf_document
---
# Hierarchical Clustering 


## Importing required libraries 
```{r}
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(ggplot2)
library(factoextra) # clustering visualization
library(dendextend) # for comparing two dendrograms
```


## Reading in Dataset
```{r}
#Read in dataset 
wine <- read.csv("wine-clustering.csv")

print(head(wine))

#Check for missing values
print(any(is.na(wine)))
```


## Standardizing the Dataset
```{r}
#Standardize dataset 
wine_std <- scale(wine)
head(wine_std)
```

# Select Optimal Number of Clusters

```{r}
fviz_nbclust(wine_std, FUN = hcut, method = "wss")
```

# Hierarchical Clustering

This portion of the project uses MIT OpenCourseWare on Youtube as reference. The information can be found at the following link: https://www.youtube.com/watch?v=GPOUGpF-Sno

We first seek to obtain the euclidean distance between all points. We use the dist() function in R. Because the dataset does not include any descriptive data such as ID number, we will use all features in our clustering algorithm. 

```{r}
distances <- dist(wine_std, method = "euclidean")
```

We now apply the hierarchical clustering method using the ward method. The ward method uses the distances between clusters using the centroid distance as well as the variance in each cluster. 

```{r}
cluster_wine <- hclust(distances, method = "complete")
summary(cluster_wine)
plot(cluster_wine)
identify(cluster_wine)
```

We now plot the dendogram of the clustering model. The dendogram lists all of the datapoints on the bottom, so this is why we see a dense black output at the bottom of the dendogram. Looking at the cluster dendogram, we can visually group three clusters in the dendogram.
```{r}
plot(cluster_wine)
rect.hclust(cluster_wine, k = 3, border = 2:5)
```


```{r}
wine_clusters <- cutree(cluster_wine, k = 3)
```

## Visualize Cluster Data
```{r}
fviz_cluster(object = list(data=wine_std, cluster = wine_clusters))
```

```{r}
table(wine_clusters, Wine_scaled)
```

Here we use code from the following link: https://www.statology.org/hierarchical-clustering-in-r/ as a second look at agglomerative clustering.

```{r}
#define linkage methods
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

#function to compute agglomerative coefficient
ac <- function(x) {
  agnes(wine_std, method = x)$ac
}

#calculate agglomerative coefficient for each clustering linkage method
sapply(m, ac)
```
We can see that the ward's minimum variance method produces the highest agglomerative coefficient, so we use this model to obtain the clustering.

```{r}
#perform hierarchical clustering using Ward's minimum variance
clust <- agnes(wine_std, method = "ward")

#produce dendrogram
pltree(clust, cex = 0.6, hang = -1, main = "Dendrogram") 

```

Obtaining the optimal number of clusters: 
```{r}
#calculate gap statistic for each number of clusters (up to 10 clusters)
gap_stat <- clusGap(wine_std, FUN = hcut, nstart = 25, K.max = 10, B = 50)

#produce plot of clusters vs. gap statistic
fviz_gap_stat(gap_stat)
```

Performing H.C using Ward's method:
```{r}
#compute distance matrix
d <- dist(wine_std, method = "euclidean")

#perform hierarchical clustering using Ward's method
final_clust <- hclust(d, method = "ward.D2" )

#cut the dendrogram into 3 clusters
groups <- cutree(final_clust, k=3)

#find number of observations in each cluster
table(groups)
```

Appending cluster labels to original dataset
```{r}
#append cluster labels to original data
final_data <- cbind(wine_std, cluster = groups)

#display first six rows of final data
head(final_data)
```

Finding the mean of variables in each cluster:
```{r}
#find mean values for each cluster
fviz_cluster(object = list(data=wine_std, cluster = groups))
```
